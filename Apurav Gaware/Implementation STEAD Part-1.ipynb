{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1529504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "#from stellargraph import StellarGraph\n",
    "#from stellargraph.layer import GAT, GCN\n",
    "\n",
    "\n",
    "# Load the STEAD dataset\n",
    "dataset_url = \"D:\\APURAV\\K. K. Wagh\\Study\\BE\\Semester VII\\Final Year Project Sem VII\\dataset\\STEAD\\merge.csv\"\n",
    "stead_data = pd.read_csv(dataset_url)\n",
    "\n",
    "\n",
    "# Data Preprocessing\n",
    "selected_columns = ['network_code', 'receiver_code', 'receiver_latitude', 'receiver_longitude',\n",
    "                    'receiver_elevation_m', 'source_latitude', 'source_longitude', 'source_depth_km',\n",
    "                    'source_magnitude', 'trace_start_time', 'trace_category']\n",
    "data = stead_data[selected_columns]\n",
    "\n",
    "\n",
    "# Convert trace_start_time to datetime format\n",
    "data['trace_start_time'] = pd.to_datetime(data['trace_start_time'])\n",
    "\n",
    "\n",
    "# Extract features from the timestamp\n",
    "data['year'] = data['trace_start_time'].dt.year\n",
    "data['month'] = data['trace_start_time'].dt.month\n",
    "data['day'] = data['trace_start_time'].dt.day\n",
    "data['hour'] = data['trace_start_time'].dt.hour\n",
    "data['minute'] = data['trace_start_time'].dt.minute\n",
    "data['second'] = data['trace_start_time'].dt.second\n",
    "\n",
    "\n",
    "# Drop unnecessary columns\n",
    "data = data.drop(['trace_start_time'], axis=1)\n",
    "\n",
    "\n",
    "# Create a binary target variable indicating earthquake or non-earthquake\n",
    "data['target'] = np.where(data['trace_category'] == 'earthquake', 1, 0)\n",
    "\n",
    "\n",
    "# Feature engineering\n",
    "# Feature 1: Time of Day (morning, afternoon, evening, night)\n",
    "data['time_of_day'] = pd.cut(data['hour'], bins=[0, 6, 12, 18, 24], labels=['night', 'morning', 'afternoon', 'evening'])\n",
    "\n",
    "# Feature 2: Distance from the earthquake source\n",
    "data['distance_from_source'] = np.sqrt((data['receiver_latitude'] - data['source_latitude'])**2 +\n",
    "                                       (data['receiver_longitude'] - data['source_longitude'])**2)\n",
    "\n",
    "# Feature 3: Magnitude-weighted distance\n",
    "data['weighted_distance'] = data['distance_from_source'] * data['source_magnitude']\n",
    "\n",
    "# Feature 4: Duration of the seismic signal\n",
    "data['signal_duration'] = data['minute'] * 60 + data['second']\n",
    "\n",
    "\n",
    "# Drop the original columns used for feature engineering\n",
    "data = data.drop(['hour', 'minute', 'second'], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# Convert categorical columns to numerical representations\n",
    "categorical_columns = ['network_code', 'receiver_code', 'time_of_day']\n",
    "for column in categorical_columns:\n",
    "    data[column] = pd.Categorical(data[column])\n",
    "    data[column] = data[column].cat.codes\n",
    "\n",
    "    \n",
    "# Create a graph from the data\n",
    "graph = StellarGraph.from_pandas(data, node_features=[\"receiver_latitude\", \"receiver_longitude\",\n",
    "                                                       \"receiver_elevation_m\", \"source_latitude\",\n",
    "                                                       \"source_longitude\", \"source_depth_km\",\n",
    "                                                       \"source_magnitude\"],\n",
    "                                 edge_features=[\"distance_from_source\", \"weighted_distance\"],\n",
    "                                 node_type_default=\"receiver_code\", edge_type_default=\"trace_category\")\n",
    "\n",
    "# Train-test split\n",
    "X = data.drop(['trace_category', 'target'], axis=1)\n",
    "y = data['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# Convert data to StellarGraph instances\n",
    "G_train = graph.node_features(X_train_scaled)\n",
    "G_test = graph.node_features(X_test_scaled)\n",
    "\n",
    "\n",
    "# Build the GNN model\n",
    "model = models.Sequential()\n",
    "model.add(GCN(layer_sizes=[32], activations=[\"relu\"], generator=graph, dropout=0.5))\n",
    "model.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "model.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "# Define callbacks (e.g., early stopping to prevent overfitting)\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.fit(G_train, y_train, epochs=50, batch_size=64, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(G_test, y_test)\n",
    "print(f'Test Accuracy: {test_accuracy}')\n",
    "\n",
    "\n",
    "# Make predictions for user input\n",
    "def predict_earthquake_probability(user_input):\n",
    "    # Process user input (similar to preprocessing steps above)\n",
    "    user_input = pd.DataFrame(user_input, index=[0])\n",
    "    \n",
    "    # Feature engineering for user input\n",
    "    \n",
    "    # Scaling\n",
    "    user_input_scaled = scaler.transform(user_input)\n",
    "    \n",
    "    # Convert to StellarGraph instances\n",
    "    G_user_input = graph.node_features(user_input_scaled)\n",
    "    \n",
    "    # Make prediction\n",
    "    probability = model.predict(G_user_input)\n",
    "    \n",
    "    return probability[0][0]\n",
    "\n",
    "\n",
    "# Example usage\n",
    "user_location_input = {\n",
    "    'network_code': 'XYZ',\n",
    "    'receiver_code': 'ABC',\n",
    "    'receiver_latitude': 37.7749,\n",
    "    'receiver_longitude': -122.4194,\n",
    "    'receiver_elevation_m': 10.0,\n",
    "    'source_latitude': 34.0522,\n",
    "    'source_longitude': -118.2437,\n",
    "    'source_depth_km': 10.0,\n",
    "    'source_magnitude': 5.0,\n",
    "    'year': 2024,\n",
    "    'month': 2,\n",
    "    'day': 5,\n",
    "    'time_of_day': 'morning'\n",
    "}\n",
    "\n",
    "predicted_probability = predict_earthquake_probability(user_location_input)\n",
    "print(f'Predicted Probability of Earthquake: {predicted_probability}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30e4dcd-7254-4fed-9877-74c937c3e448",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91e3f44-d813-4a56-976a-9d64d37af354",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ca6fc00",
   "metadata": {},
   "source": [
    "2nd try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af42b8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"D:\\APURAV\\K. K. Wagh\\Study\\BE\\Semester VII\\Final Year Project Sem VII\\dataset\\STEAD\\merge.csv\")\n",
    "\n",
    "# Explore the dataset\n",
    "print(data.head())\n",
    "print(data.info())\n",
    "print(data.describe())\n",
    "\n",
    "# Handle missing values\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Handle outliers (if necessary)\n",
    "# Perform data consistency checks and corrections\n",
    "\n",
    "# Feature engineering\n",
    "# Example:\n",
    "# Convert source_origin_time to datetime\n",
    "data['source_origin_time'] = pd.to_datetime(data['source_origin_time'])\n",
    "\n",
    "# Scaling, encoding, dimensionality reduction (if necessary)\n",
    "# Example:\n",
    "# Feature scaling using Min-Max normalization\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data[['source_latitude', 'source_longitude']])\n",
    "\n",
    "# Convert scaled_data back to a DataFrame\n",
    "scaled_data_df = pd.DataFrame(scaled_data, columns=['source_latitude', 'source_longitude'])\n",
    "\n",
    "# Dimensionality reduction using PCA (Principal Component Analysis) or other methods if necessary\n",
    "\n",
    "# Save preprocessed data\n",
    "scaled_data_df.to_csv(\"preprocessed_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509324e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement GNN architecture using TensorFlow or PyTorch\n",
    "# Example:\n",
    "\n",
    "class GNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GNN, self).__init__()\n",
    "        # Define GNN layers\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Define forward pass\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate GNN model\n",
    "input_dim = 2  # Example: latitude and longitude\n",
    "hidden_dim = 64\n",
    "output_dim = 1  # Probability of earthquake occurrence\n",
    "model = GNN(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40b5e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Visualize seismic data distributions\n",
    "sns.pairplot(data[['source_latitude', 'source_longitude', 'source_depth_km']])\n",
    "plt.show()\n",
    "\n",
    "# Display GNN architecture diagrams (if necessary)\n",
    "\n",
    "# Use interactive plots for seismic waveforms and earthquake characteristics (if necessary)\n",
    "# Example: Plot seismic waveforms over time using Plotly or Bokeh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293883bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049bf3b6-b469-47a0-a006-da56fc4c2017",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5213d8b",
   "metadata": {},
   "source": [
    "3rd try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639f2f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"D:\\APURAV\\K. K. Wagh\\Study\\BE\\Semester VII\\Final Year Project Sem VII\\dataset\\STEAD\\merge.csv\")\n",
    "\n",
    "# Drop unnecessary columns\n",
    "data.drop(['network_code', 'receiver_code', 'receiver_type', 'receiver_elevation_m',\n",
    "           'p_status', 'p_weight', 'p_travel_sec', 's_status', 's_weight', 'source_id',\n",
    "           'source_origin_uncertainty_sec', 'source_error_sec', 'source_gap_deg',\n",
    "           'source_horizontal_uncertainty_km', 'source_depth_uncertainty_km',\n",
    "           'source_magnitude_type', 'source_magnitude_author', 'source_mechanism_strike_dip_rake',\n",
    "           'source_distance_deg', 'trace_start_time', 'trace_category', 'trace_name'], axis=1, inplace=True)\n",
    "\n",
    "# Convert timestamp columns to datetime objects\n",
    "data['source_origin_time'] = pd.to_datetime(data['source_origin_time'])\n",
    "\n",
    "# Extract features from timestamp columns\n",
    "data['hour'] = data['source_origin_time'].dt.hour\n",
    "data['minute'] = data['source_origin_time'].dt.minute\n",
    "data['second'] = data['source_origin_time'].dt.second\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(data[['hour', 'minute', 'second']])\n",
    "data[['hour', 'minute', 'second']] = scaled_features\n",
    "\n",
    "# Handle missing values\n",
    "data.fillna(0, inplace=True)\n",
    "\n",
    "# Encode categorical variables if needed\n",
    "\n",
    "# Final processed dataset\n",
    "processed_data = data.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a31f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolutionalNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GraphConvolutionalNetwork, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        return x\n",
    "\n",
    "# Initialize the GNN model\n",
    "input_dim = len(processed_data.columns)  # Adjust based on your processed data\n",
    "hidden_dim = 64\n",
    "output_dim = 1  # Adjust based on your prediction task\n",
    "model = GraphConvolutionalNetwork(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f40f0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
