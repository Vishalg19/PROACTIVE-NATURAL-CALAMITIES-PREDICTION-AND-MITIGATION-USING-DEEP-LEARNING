{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6902fbc1-aa84-4862-8c7d-acd93800c83d",
   "metadata": {},
   "source": [
    "4th try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d03ac25-e108-40b7-a1ad-5d437e684366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Step 1: Data Understanding and Preprocessing\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"D:\\APURAV\\K. K. Wagh\\Study\\BE\\Semester VII\\Final Year Project Sem VII\\dataset\\STEAD\\merge.csv\")\n",
    "\n",
    "# Review the dataset\n",
    "print(data.head())\n",
    "\n",
    "# Handle missing values\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Handle outliers\n",
    "# You may use techniques like Z-score, IQR, or domain-specific knowledge\n",
    "\n",
    "# Feature engineering\n",
    "# Extract relevant features\n",
    "# Example: Extracting seismic waveforms into separate arrays\n",
    "seismic_waveforms = data[['trace_start_time', 'trace_category', 'trace_name']]\n",
    "\n",
    "# Data encoding and scaling\n",
    "# Example: Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "data[['source_magnitude', 'source_depth_km']] = scaler.fit_transform(data[['source_magnitude', 'source_depth_km']])\n",
    "\n",
    "# Dimensionality reduction\n",
    "# You may use techniques like PCA if needed\n",
    "X = data.drop(columns=['trace_category'])\n",
    "y = data['trace_category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76b1b74-115c-4e29-9176-d493c90509d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GNNModel, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        \n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Instantiate the GNN model\n",
    "input_dim = len(X.columns)  # Adjust input dimension based on the number of features\n",
    "hidden_dim = 64\n",
    "output_dim = 2  # Assuming binary classification for earthquake prediction\n",
    "model = GNNModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fda29cc-f66c-465f-b10d-f0eac486d387",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Visualize data distributions, feature relationships, model predictions, etc.\n",
    "\n",
    "# For example, visualize the correlation matrix of features\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(data.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Visualize model performance metrics (e.g., accuracy, loss) during training\n",
    "def plot_metrics(train_losses, val_losses, train_accuracies, val_accuracies):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.plot(epochs, train_losses, label='Train Loss')\n",
    "    plt.plot(epochs, val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs, train_accuracies, label='Train Accuracy')\n",
    "    plt.plot(epochs, val_accuracies, label='Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Usage: Call plot_metrics with training and validation metrics lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4e52aa-0743-4947-a2d1-326a1aee5d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=10):\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accuracies, val_accuracies = [], []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        \n",
    "        for data in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, data.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_train += data.y.size(0)\n",
    "            correct_train += (predicted == data.y).sum().item()\n",
    "        \n",
    "        train_losses.append(running_loss / len(train_loader))\n",
    "        train_accuracies.append(correct_train / total_train)\n",
    "        \n",
    "        val_loss, val_acc = evaluate_model(model, criterion, val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "        \n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "              f'Train Loss: {train_losses[-1]:.4f}, Train Acc: {train_accuracies[-1]:.4f}, '\n",
    "              f'Val Loss: {val_losses[-1]:.4f}, Val Acc: {val_accuracies[-1]:.4f}')\n",
    "    \n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate_model(model, criterion, data_loader):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, data.y)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += data.y.size(0)\n",
    "            correct += (predicted == data.y).sum().item()\n",
    "    \n",
    "    return running_loss / len(data_loader), correct / total\n",
    "\n",
    "# Make predictions\n",
    "def predict(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(data)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "    return predicted\n",
    "\n",
    "# Usage: train the model, evaluate it on validation data, and make predictions on test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a800311-6234-4421-9284-9ff40cda4d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X is your DataFrame with features\n",
    "# Identify categorical columns\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Perform feature hashing on categorical columns\n",
    "hasher = FeatureHasher(n_features=10, input_type='string')\n",
    "X_hashed = hasher.transform(X[categorical_cols].astype(str))\n",
    "\n",
    "# Convert hashed features to a DataFrame\n",
    "X_hashed_df = pd.DataFrame(X_hashed.toarray())\n",
    "\n",
    "# Concatenate hashed features with numerical features\n",
    "X_processed = pd.concat([X.drop(columns=categorical_cols), X_hashed_df], axis=1)\n",
    "\n",
    "# Assuming X_processed is your DataFrame with processed features\n",
    "# Check the dimensions of X_processed and y\n",
    "print(\"Dimensions of X_processed:\", X_processed.shape)\n",
    "print(\"Dimensions of y:\", y.shape)\n",
    "\n",
    "# Find the indices of rows that are present in X_processed but not in y\n",
    "missing_indices = set(X_processed.index) - set(y.index)\n",
    "print(\"Missing indices:\", missing_indices)\n",
    "\n",
    "# Find the indices of rows that are present in y but not in X_processed\n",
    "extra_indices = set(y.index) - set(X_processed.index)\n",
    "print(\"Extra indices:\", extra_indices)\n",
    "\n",
    "# Check if the number of samples in X_processed and y is consistent\n",
    "if len(X_processed) != len(y):\n",
    "    # If there are extra indices in y, drop those rows\n",
    "    if extra_indices:\n",
    "        print(\"Dropping extra indices in y:\", extra_indices)\n",
    "        y.drop(index=extra_indices, inplace=True)\n",
    "    \n",
    "    # If there are missing indices in y, drop those rows from X_processed\n",
    "    if missing_indices:\n",
    "        print(\"Dropping missing indices in X_processed:\", missing_indices)\n",
    "        X_processed.drop(index=missing_indices, inplace=True)\n",
    "    \n",
    "    # Check again for consistency\n",
    "    if len(X_processed) != len(y):\n",
    "        raise ValueError(\"Number of samples in features and target are not consistent!\")\n",
    "\n",
    "# Ensure that the indices of X_processed and y are aligned\n",
    "X_processed.reset_index(drop=True, inplace=True)\n",
    "y.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Concatenate X_processed and y along the columns axis\n",
    "data = pd.concat([X_processed, y], axis=1)\n",
    "\n",
    "\n",
    "# Assuming y is your target variable\n",
    "# Encode categorical labels in y\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "X_train, X_temp, y_train_encoded, y_temp_encoded = train_test_split(X_processed, y_encoded, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val_encoded, y_test_encoded = train_test_split(X_temp, y_temp_encoded, test_size=0.5, random_state=42)\n",
    "\n",
    "# Convert data to appropriate numeric types\n",
    "X_train_values = X_train.values.astype(np.float32)\n",
    "y_train_values = y_train_encoded.astype(np.int64)\n",
    "X_val_values = X_val.values.astype(np.float32)\n",
    "y_val_values = y_val_encoded.astype(np.int64)\n",
    "X_test_values = X_test.values.astype(np.float32)\n",
    "y_test_values = y_test_encoded.astype(np.int64)\n",
    "\n",
    "# Assuming train_edge_index, val_edge_index, and test_edge_index are properly defined\n",
    "# Define the data objects\n",
    "train_data = Data.Data(x=torch.tensor(X_train_values),\n",
    "                       y=torch.tensor(y_train_values),\n",
    "                       edge_index=torch.tensor(train_edge_index, dtype=torch.long))\n",
    "val_data = Data.Data(x=torch.tensor(X_val_values),\n",
    "                     y=torch.tensor(y_val_values),\n",
    "                     edge_index=torch.tensor(val_edge_index, dtype=torch.long))\n",
    "test_data = Data.Data(x=torch.tensor(X_test_values),\n",
    "                      y=torch.tensor(y_test_values),\n",
    "                      edge_index=torch.tensor(test_edge_index, dtype=torch.long))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaad6deb-278b-48b9-bd24-e38fa07b89ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862575c2-9c27-46ce-a02e-2e6073318df0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96bc9e70-09ea-48fd-aefc-e87715037463",
   "metadata": {},
   "source": [
    "5th try version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05916978-5786-4a6a-bc98-ffc0b76bb7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"D:/APURAV/K. K. Wagh/Study/BE/Semester VII/Final Year Project Sem VII/dataset/STEAD/merge.csv\")\n",
    "\n",
    "# Explore the dataset\n",
    "print(data.head())\n",
    "print(data.info())\n",
    "\n",
    "# Handle missing values\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Feature engineering\n",
    "categorical_cols = data.select_dtypes(include=['object']).columns\n",
    "hasher = FeatureHasher(n_features=10, input_type='string')\n",
    "hashed_features = hasher.transform(data[categorical_cols].astype(str)).toarray()\n",
    "hashed_df = pd.DataFrame(hashed_features)\n",
    "processed_data = pd.concat([data.drop(columns=categorical_cols), hashed_df], axis=1)\n",
    "\n",
    "# Encoding categorical labels in y\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(data['trace_category'])\n",
    "\n",
    "# Ensure consistency in the number of samples\n",
    "processed_data = processed_data.iloc[:len(y_encoded)]\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(processed_data, y_encoded, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Data preprocessing\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert data to appropriate tensor types\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Define the GCN model\n",
    "class GNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GNNModel, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Instantiate the model\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "hidden_dim = 64\n",
    "output_dim = len(label_encoder.classes_)\n",
    "model = GNNModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Define data objects\n",
    "train_data = Data(x=X_train_tensor, y=y_train_tensor)\n",
    "val_data = Data(x=X_val_tensor, y=y_val_tensor)\n",
    "test_data = Data(x=X_test_tensor, y=y_test_tensor)\n",
    "\n",
    "# Print dimensions\n",
    "print(\"Dimensions of train_data:\", len(train_data), train_data.num_node_features, train_data.num_edge_features)\n",
    "print(\"Dimensions of val_data:\", len(val_data), val_data.num_node_features, val_data.num_edge_features)\n",
    "print(\"Dimensions of test_data:\", len(test_data), test_data.num_node_features, test_data.num_edge_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf24d766-43ea-4ccc-a1de-2caaa3de19c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32228d7f-effd-4137-b6ad-4de71be42689",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c981a522-675f-44b9-8ac5-f7c6992f2e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.0.0+cu118.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96538779-2aea-4498-9464-577085e79d16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3aa3b96-0df2-4cd7-b1be-404869f7b995",
   "metadata": {},
   "source": [
    "5th try version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9114a412-a38f-4b3f-bcc9-fd23e0669e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader  # Import DataLoader from torch.utils.data\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"D:/APURAV/K. K. Wagh/Study/BE/Semester VII/Final Year Project Sem VII/dataset/STEAD/merge.csv\")\n",
    "\n",
    "# Explore the dataset\n",
    "print(data.head())\n",
    "print(data.info())\n",
    "\n",
    "# Handle missing values\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Feature engineering\n",
    "categorical_cols = data.select_dtypes(include=['object']).columns\n",
    "hasher = FeatureHasher(n_features=10, input_type='string')\n",
    "hashed_features = hasher.transform(data[categorical_cols].astype(str)).toarray()\n",
    "hashed_df = pd.DataFrame(hashed_features)\n",
    "processed_data = pd.concat([data.drop(columns=categorical_cols), hashed_df], axis=1)\n",
    "\n",
    "# Encoding categorical labels in y\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(data['trace_category'])\n",
    "\n",
    "# Ensure consistency in the number of samples\n",
    "processed_data = processed_data.iloc[:len(y_encoded)]\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(processed_data, y_encoded, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Data preprocessing\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert data to appropriate tensor types\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Define the GCN model\n",
    "class GNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GNNModel, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index=None):\n",
    "        if edge_index is not None:\n",
    "            x = F.relu(self.conv1(x, edge_index))\n",
    "        else:\n",
    "            x = F.relu(self.conv1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Instantiate the model\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "hidden_dim = 64\n",
    "output_dim = len(label_encoder.classes_)\n",
    "model = GNNModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Define data objects\n",
    "train_data = [Data(x=X_train_tensor, edge_index=None, y=y_train_tensor)]  # Include edge_index=None\n",
    "val_data = [Data(x=X_val_tensor, edge_index=None, y=y_val_tensor)]  # Include edge_index=None\n",
    "test_data = [Data(x=X_test_tensor, edge_index=None, y=y_test_tensor)]  # Include edge_index=None\n",
    "\n",
    "# Print dimensions\n",
    "print(\"Dimensions of train_data:\", len(train_data), train_data[0].num_node_features, train_data[0].num_edge_features)\n",
    "print(\"Dimensions of val_data:\", len(val_data), val_data[0].num_node_features, val_data[0].num_edge_features)\n",
    "print(\"Dimensions of test_data:\", len(test_data), test_data[0].num_node_features, test_data[0].num_edge_features)\n",
    "\n",
    "# Define the training parameters\n",
    "num_epochs = 20\n",
    "batch_size = 64\n",
    "\n",
    "# Create data loaders using torch.utils.data.DataLoader\n",
    "def collate(data_list):\n",
    "    X = [data.x for data in data_list]\n",
    "    y = [data.y for data in data_list]\n",
    "    return Data(x=torch.stack(X), edge_index=None, y=torch.stack(y))  # Include edge_index=None\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, collate_fn=collate)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, collate_fn=collate)\n",
    "\n",
    "# Rest of the code remains unchanged\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, device, num_epochs=10):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            data = batch.to(device)  # Move data to the appropriate device\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, data.y.view(-1))  # Assuming 'y' is the target label\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_train += (predicted == data.y.view(-1)).sum().item()\n",
    "            total_train += data.y.size(0)\n",
    "        \n",
    "        train_losses.append(train_loss / len(train_loader))\n",
    "        train_accuracies.append(correct_train / total_train)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                data = batch.to(device)  # Move data to the appropriate device\n",
    "                outputs = model(data)\n",
    "                loss = criterion(outputs, data.y.view(-1))  # Assuming 'y' is the target label\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct_val += (predicted == data.y.view(-1)).sum().item()\n",
    "                total_val += data.y.size(0)\n",
    "        \n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        val_accuracies.append(correct_val / total_val)\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_losses[-1]:.4f}, Train Acc: {train_accuracies[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}, Val Acc: {val_accuracies[-1]:.4f}')\n",
    "    \n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies\n",
    "\n",
    "def evaluate_model(model, criterion, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, data.y)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == data.y).sum().item()\n",
    "            total += data.y.size(0)\n",
    "    \n",
    "    loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    return loss, accuracy\n",
    "\n",
    "def predict(model, data_loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            predictions.extend(predicted.tolist())\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Assuming you have the train_model, evaluate_model, and predict functions defined as shown above,\n",
    "# you can then proceed to use these functions with your model.\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Train the model\n",
    "train_losses, val_losses, train_accuracies, val_accuracies = train_model(model, criterion, optimizer, train_loader, val_loader, device=device, num_epochs=num_epochs)\n",
    "\n",
    "# Evaluate the model on the test set (assuming you have defined the evaluate_model function elsewhere)\n",
    "test_loss, test_accuracy = evaluate_model(model, criterion, test_loader)\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = []\n",
    "ground_truth = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        outputs = predict(model, data)\n",
    "        predictions.extend(outputs.tolist())\n",
    "        ground_truth.extend(data.y.tolist())\n",
    "\n",
    "# Convert predictions and ground truth to numpy arrays\n",
    "predictions = np.array(predictions)\n",
    "ground_truth = np.array(ground_truth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c0103f-c59a-4b66-a0aa-51122601ba98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb93c66-9710-461a-b104-fe262f71cf82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a975420-210d-4e3d-8c86-135275609f8c",
   "metadata": {},
   "source": [
    "5th try version 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c940ab7a-5ef4-4514-af98-251d2463b8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_sparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"D:/APURAV/K. K. Wagh/Study/BE/Semester VII/Final Year Project Sem VII/dataset/STEAD/merge.csv\")\n",
    "\n",
    "# Explore the dataset\n",
    "print(data.head())\n",
    "print(data.info())\n",
    "\n",
    "# Handle missing values\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Feature engineering\n",
    "categorical_cols = data.select_dtypes(include=['object']).columns\n",
    "hasher = FeatureHasher(n_features=10, input_type='string')\n",
    "hashed_features = hasher.transform(data[categorical_cols].astype(str)).toarray()\n",
    "hashed_df = pd.DataFrame(hashed_features)\n",
    "processed_data = pd.concat([data.drop(columns=categorical_cols), hashed_df], axis=1)\n",
    "\n",
    "# Encoding categorical labels in y\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(data['trace_category'])\n",
    "\n",
    "# Ensure consistency in the number of samples\n",
    "processed_data = processed_data.iloc[:len(y_encoded)]\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(processed_data, y_encoded, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Data preprocessing\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert data to appropriate tensor types\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Update the forward method of GNNModel\n",
    "class GNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GNNModel, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Update the training loop\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, device, num_epochs=10):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        \n",
    "        for data in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            data = data.to(device)  # Move data to the appropriate device\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, data.y.view(-1))  # Assuming 'y' is the target label\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_train += (predicted == data.y.view(-1)).sum().item()\n",
    "            total_train += data.y.size(0)\n",
    "        \n",
    "        train_losses.append(train_loss / len(train_loader))\n",
    "        train_accuracies.append(correct_train / total_train)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                data = data.to(device)  # Move data to the appropriate device\n",
    "                outputs = model(data)\n",
    "                loss = criterion(outputs, data.y.view(-1))  # Assuming 'y' is the target label\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct_val += (predicted == data.y.view(-1)).sum().item()\n",
    "                total_val += data.y.size(0)\n",
    "        \n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        val_accuracies.append(correct_val / total_val)\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_losses[-1]:.4f}, Train Acc: {train_accuracies[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}, Val Acc: {val_accuracies[-1]:.4f}')\n",
    "    \n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies\n",
    "\n",
    "\n",
    "# Instantiate the model\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "hidden_dim = 64\n",
    "output_dim = len(label_encoder.classes_)\n",
    "model = GNNModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Define data objects\n",
    "train_data = [Data(x=X_train_tensor, edge_index=None, y=y_train_tensor)]\n",
    "val_data = [Data(x=X_val_tensor, edge_index=None, y=y_val_tensor)]\n",
    "test_data = [Data(x=X_test_tensor, edge_index=None, y=y_test_tensor)]\n",
    "\n",
    "# Print dimensions\n",
    "print(\"Dimensions of train_data:\", len(train_data), train_data[0].num_node_features, train_data[0].num_edge_features)\n",
    "print(\"Dimensions of val_data:\", len(val_data), val_data[0].num_node_features, val_data[0].num_edge_features)\n",
    "print(\"Dimensions of test_data:\", len(test_data), test_data[0].num_node_features, test_data[0].num_edge_features)\n",
    "\n",
    "# Define the training parameters\n",
    "num_epochs = 20\n",
    "batch_size = 64\n",
    "\n",
    "# Create data loaders using torch.utils.data.DataLoader\n",
    "def collate(data_list):\n",
    "    X = [data.x for data in data_list]\n",
    "    y = [data.y for data in data_list]\n",
    "    return Data(x=torch.stack(X), edge_index=None, y=torch.stack(y))\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, collate_fn=collate)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, collate_fn=collate)\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Train the model\n",
    "train_losses, val_losses, train_accuracies, val_accuracies = train_model(model, criterion, optimizer, train_loader, val_loader, device=device, num_epochs=num_epochs)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = evaluate_model(model, criterion, test_loader)\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = []\n",
    "ground_truth = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        outputs = predict(model, data)\n",
    "        predictions.extend(outputs.tolist())\n",
    "        ground_truth.extend(data.y.tolist())\n",
    "\n",
    "# Convert predictions and ground truth to numpy arrays\n",
    "predictions = np.array(predictions)\n",
    "ground_truth = np.array(ground_truth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa285f88-f573-499c-8878-ef7d1e67fa91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccefeb2f-2f00-4b66-b70b-17042258271c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5736ef7-0596-43c3-accd-71065cac2de0",
   "metadata": {},
   "source": [
    "5th try version 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a21772-bc5b-4fb6-ba38-54c1e31f2554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_scatter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"D:/APURAV/K. K. Wagh/Study/BE/Semester VII/Final Year Project Sem VII/dataset/STEAD/merge.csv\")\n",
    "\n",
    "# Explore the dataset\n",
    "print(data.head())\n",
    "print(data.info())\n",
    "\n",
    "# Handle missing values\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Feature engineering\n",
    "categorical_cols = data.select_dtypes(include=['object']).columns\n",
    "hasher = FeatureHasher(n_features=10, input_type='string')\n",
    "hashed_features = hasher.transform(data[categorical_cols].astype(str)).toarray()\n",
    "hashed_df = pd.DataFrame(hashed_features)\n",
    "processed_data = pd.concat([data.drop(columns=categorical_cols), hashed_df], axis=1)\n",
    "\n",
    "# Encoding categorical labels in y\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(data['trace_category'])\n",
    "\n",
    "# Ensure consistency in the number of samples\n",
    "processed_data = processed_data.iloc[:len(y_encoded)]\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(processed_data, y_encoded, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Data preprocessing\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert data to appropriate tensor types\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Define the GCN model\n",
    "class GNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GNNModel, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        if edge_index is None:\n",
    "            x = F.relu(self.conv1(x, torch.zeros(2, 0)))  # Provide a dummy edge_index when it's None\n",
    "        else:\n",
    "            x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Instantiate the model\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "hidden_dim = 64\n",
    "output_dim = len(label_encoder.classes_)\n",
    "model = GNNModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Define data objects\n",
    "train_data = [Data(x=X_train_tensor, edge_index=None, y=y_train_tensor)]\n",
    "val_data = [Data(x=X_val_tensor, edge_index=None, y=y_val_tensor)]\n",
    "test_data = [Data(x=X_test_tensor, edge_index=None, y=y_test_tensor)]\n",
    "\n",
    "# Print dimensions\n",
    "print(\"Dimensions of train_data:\", len(train_data), train_data[0].num_node_features, train_data[0].num_edge_features)\n",
    "print(\"Dimensions of val_data:\", len(val_data), val_data[0].num_node_features, val_data[0].num_edge_features)\n",
    "print(\"Dimensions of test_data:\", len(test_data), test_data[0].num_node_features, test_data[0].num_edge_features)\n",
    "\n",
    "# Define the training parameters\n",
    "num_epochs = 20\n",
    "batch_size = 64\n",
    "\n",
    "# Create data loaders using torch.utils.data.DataLoader\n",
    "def collate(data_list):\n",
    "    X = [data.x for data in data_list]\n",
    "    y = [data.y for data in data_list]\n",
    "    return Data(x=torch.stack(X), edge_index=None, y=torch.stack(y))\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, collate_fn=collate)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, collate_fn=collate)\n",
    "\n",
    "# Define the training function\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, device, num_epochs):\n",
    "    # Move model to the appropriate device\n",
    "    model.to(device)\n",
    "    # Set model to train mode\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_train = 0\n",
    "        correct_train = 0\n",
    "        running_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            data = batch.to(device)  # Move data to the appropriate device\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, data.y.view(-1))  # Assuming 'y' is the target label\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_train += data.y.size(0)\n",
    "            correct_train += (predicted == data.y).sum().item()\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_accuracy = correct_train / total_train\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        total_val = 0\n",
    "        correct_val = 0\n",
    "        val_running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                data = batch.to(device)  # Move data to the appropriate device\n",
    "                outputs = model(data)\n",
    "                val_loss = criterion(outputs, data.y.view(-1))  # Assuming 'y' is the target label\n",
    "                val_running_loss += val_loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total_val += data.y.size(0)\n",
    "                correct_val += (predicted == data.y).sum().item()\n",
    "            val_loss = val_running_loss / len(val_loader)\n",
    "            val_accuracy = correct_val / total_val\n",
    "            val_losses.append(val_loss)\n",
    "            val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "        \n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies\n",
    "\n",
    "def evaluate_model(model, criterion, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, data.y)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == data.y).sum().item()\n",
    "            total += data.y.size(0)\n",
    "    \n",
    "    loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    return loss, accuracy\n",
    "\n",
    "def predict(model, data_loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            predictions.extend(predicted.tolist())\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Assuming you have the train_model, evaluate_model, and predict functions defined as shown above,\n",
    "# you can then proceed to use these functions with your model.\n",
    "\n",
    "# Train the model\n",
    "train_losses, val_losses, train_accuracies, val_accuracies = train_model(model, criterion, optimizer, train_loader, val_loader, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'), num_epochs=num_epochs)\n",
    "\n",
    "# Evaluate the model on the test set (assuming you have defined the evaluate_model function elsewhere)\n",
    "test_loss, test_accuracy = evaluate_model(model, criterion, test_loader)\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = []\n",
    "ground_truth = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        outputs = predict(model, data)\n",
    "        predictions.extend(outputs.tolist())\n",
    "        ground_truth.extend(data.y.tolist())\n",
    "\n",
    "# Convert predictions and ground truth to numpy arrays\n",
    "predictions = np.array(predictions)\n",
    "ground_truth = np.array(ground_truth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfea89b5-2b9c-45b9-be52-2dbac96408ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
