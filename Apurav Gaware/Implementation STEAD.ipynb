{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1529504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Import necessary libraries\\nimport numpy as np\\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import StandardScaler\\nfrom tensorflow import keras\\nfrom tensorflow.keras import layers, models, callbacks\\n#from stellargraph import StellarGraph\\n#from stellargraph.layer import GAT, GCN\\n\\n\\n# Load the STEAD dataset\\ndataset_url = \"D:\\\\APURAV\\\\K. K. Wagh\\\\Study\\\\BE\\\\Semester VII\\\\Final Year Project Sem VII\\\\dataset\\\\STEAD\\\\merge.csv\"\\nstead_data = pd.read_csv(dataset_url)\\n\\n\\n# Data Preprocessing\\nselected_columns = [\\'network_code\\', \\'receiver_code\\', \\'receiver_latitude\\', \\'receiver_longitude\\',\\n                    \\'receiver_elevation_m\\', \\'source_latitude\\', \\'source_longitude\\', \\'source_depth_km\\',\\n                    \\'source_magnitude\\', \\'trace_start_time\\', \\'trace_category\\']\\ndata = stead_data[selected_columns]\\n\\n\\n# Convert trace_start_time to datetime format\\ndata[\\'trace_start_time\\'] = pd.to_datetime(data[\\'trace_start_time\\'])\\n\\n\\n# Extract features from the timestamp\\ndata[\\'year\\'] = data[\\'trace_start_time\\'].dt.year\\ndata[\\'month\\'] = data[\\'trace_start_time\\'].dt.month\\ndata[\\'day\\'] = data[\\'trace_start_time\\'].dt.day\\ndata[\\'hour\\'] = data[\\'trace_start_time\\'].dt.hour\\ndata[\\'minute\\'] = data[\\'trace_start_time\\'].dt.minute\\ndata[\\'second\\'] = data[\\'trace_start_time\\'].dt.second\\n\\n\\n# Drop unnecessary columns\\ndata = data.drop([\\'trace_start_time\\'], axis=1)\\n\\n\\n# Create a binary target variable indicating earthquake or non-earthquake\\ndata[\\'target\\'] = np.where(data[\\'trace_category\\'] == \\'earthquake\\', 1, 0)\\n\\n\\n# Feature engineering\\n# Feature 1: Time of Day (morning, afternoon, evening, night)\\ndata[\\'time_of_day\\'] = pd.cut(data[\\'hour\\'], bins=[0, 6, 12, 18, 24], labels=[\\'night\\', \\'morning\\', \\'afternoon\\', \\'evening\\'])\\n\\n# Feature 2: Distance from the earthquake source\\ndata[\\'distance_from_source\\'] = np.sqrt((data[\\'receiver_latitude\\'] - data[\\'source_latitude\\'])**2 +\\n                                       (data[\\'receiver_longitude\\'] - data[\\'source_longitude\\'])**2)\\n\\n# Feature 3: Magnitude-weighted distance\\ndata[\\'weighted_distance\\'] = data[\\'distance_from_source\\'] * data[\\'source_magnitude\\']\\n\\n# Feature 4: Duration of the seismic signal\\ndata[\\'signal_duration\\'] = data[\\'minute\\'] * 60 + data[\\'second\\']\\n\\n\\n# Drop the original columns used for feature engineering\\ndata = data.drop([\\'hour\\', \\'minute\\', \\'second\\'], axis=1)\\n\\n\\n\\n# Convert categorical columns to numerical representations\\ncategorical_columns = [\\'network_code\\', \\'receiver_code\\', \\'time_of_day\\']\\nfor column in categorical_columns:\\n    data[column] = pd.Categorical(data[column])\\n    data[column] = data[column].cat.codes\\n\\n    \\n# Create a graph from the data\\ngraph = StellarGraph.from_pandas(data, node_features=[\"receiver_latitude\", \"receiver_longitude\",\\n                                                       \"receiver_elevation_m\", \"source_latitude\",\\n                                                       \"source_longitude\", \"source_depth_km\",\\n                                                       \"source_magnitude\"],\\n                                 edge_features=[\"distance_from_source\", \"weighted_distance\"],\\n                                 node_type_default=\"receiver_code\", edge_type_default=\"trace_category\")\\n\\n# Train-test split\\nX = data.drop([\\'trace_category\\', \\'target\\'], axis=1)\\ny = data[\\'target\\']\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n\\n# Feature Scaling\\nscaler = StandardScaler()\\nX_train_scaled = scaler.fit_transform(X_train)\\nX_test_scaled = scaler.transform(X_test)\\n\\n\\n# Convert data to StellarGraph instances\\nG_train = graph.node_features(X_train_scaled)\\nG_test = graph.node_features(X_test_scaled)\\n\\n\\n# Build the GNN model\\nmodel = models.Sequential()\\nmodel.add(GCN(layer_sizes=[32], activations=[\"relu\"], generator=graph, dropout=0.5))\\nmodel.add(layers.Dense(units=16, activation=\"relu\"))\\nmodel.add(layers.Dense(units=1, activation=\"sigmoid\"))\\n\\n\\n# Compile the model\\nmodel.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\\n\\n\\n# Define callbacks (e.g., early stopping to prevent overfitting)\\nearly_stopping = callbacks.EarlyStopping(monitor=\\'val_loss\\', patience=10, restore_best_weights=True)\\n\\n\\n# Train the model\\nmodel.fit(G_train, y_train, epochs=50, batch_size=64, validation_split=0.2, callbacks=[early_stopping])\\n\\n\\n# Evaluate the model on the test set\\ntest_loss, test_accuracy = model.evaluate(G_test, y_test)\\nprint(f\\'Test Accuracy: {test_accuracy}\\')\\n\\n\\n# Make predictions for user input\\ndef predict_earthquake_probability(user_input):\\n    # Process user input (similar to preprocessing steps above)\\n    user_input = pd.DataFrame(user_input, index=[0])\\n    \\n    # Feature engineering for user input\\n    \\n    # Scaling\\n    user_input_scaled = scaler.transform(user_input)\\n    \\n    # Convert to StellarGraph instances\\n    G_user_input = graph.node_features(user_input_scaled)\\n    \\n    # Make prediction\\n    probability = model.predict(G_user_input)\\n    \\n    return probability[0][0]\\n\\n\\n# Example usage\\nuser_location_input = {\\n    \\'network_code\\': \\'XYZ\\',\\n    \\'receiver_code\\': \\'ABC\\',\\n    \\'receiver_latitude\\': 37.7749,\\n    \\'receiver_longitude\\': -122.4194,\\n    \\'receiver_elevation_m\\': 10.0,\\n    \\'source_latitude\\': 34.0522,\\n    \\'source_longitude\\': -118.2437,\\n    \\'source_depth_km\\': 10.0,\\n    \\'source_magnitude\\': 5.0,\\n    \\'year\\': 2024,\\n    \\'month\\': 2,\\n    \\'day\\': 5,\\n    \\'time_of_day\\': \\'morning\\'\\n}\\n\\npredicted_probability = predict_earthquake_probability(user_location_input)\\nprint(f\\'Predicted Probability of Earthquake: {predicted_probability}\\')\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "#from stellargraph import StellarGraph\n",
    "#from stellargraph.layer import GAT, GCN\n",
    "\n",
    "\n",
    "# Load the STEAD dataset\n",
    "dataset_url = \"D:\\APURAV\\K. K. Wagh\\Study\\BE\\Semester VII\\Final Year Project Sem VII\\dataset\\STEAD\\merge.csv\"\n",
    "stead_data = pd.read_csv(dataset_url)\n",
    "\n",
    "\n",
    "# Data Preprocessing\n",
    "selected_columns = ['network_code', 'receiver_code', 'receiver_latitude', 'receiver_longitude',\n",
    "                    'receiver_elevation_m', 'source_latitude', 'source_longitude', 'source_depth_km',\n",
    "                    'source_magnitude', 'trace_start_time', 'trace_category']\n",
    "data = stead_data[selected_columns]\n",
    "\n",
    "\n",
    "# Convert trace_start_time to datetime format\n",
    "data['trace_start_time'] = pd.to_datetime(data['trace_start_time'])\n",
    "\n",
    "\n",
    "# Extract features from the timestamp\n",
    "data['year'] = data['trace_start_time'].dt.year\n",
    "data['month'] = data['trace_start_time'].dt.month\n",
    "data['day'] = data['trace_start_time'].dt.day\n",
    "data['hour'] = data['trace_start_time'].dt.hour\n",
    "data['minute'] = data['trace_start_time'].dt.minute\n",
    "data['second'] = data['trace_start_time'].dt.second\n",
    "\n",
    "\n",
    "# Drop unnecessary columns\n",
    "data = data.drop(['trace_start_time'], axis=1)\n",
    "\n",
    "\n",
    "# Create a binary target variable indicating earthquake or non-earthquake\n",
    "data['target'] = np.where(data['trace_category'] == 'earthquake', 1, 0)\n",
    "\n",
    "\n",
    "# Feature engineering\n",
    "# Feature 1: Time of Day (morning, afternoon, evening, night)\n",
    "data['time_of_day'] = pd.cut(data['hour'], bins=[0, 6, 12, 18, 24], labels=['night', 'morning', 'afternoon', 'evening'])\n",
    "\n",
    "# Feature 2: Distance from the earthquake source\n",
    "data['distance_from_source'] = np.sqrt((data['receiver_latitude'] - data['source_latitude'])**2 +\n",
    "                                       (data['receiver_longitude'] - data['source_longitude'])**2)\n",
    "\n",
    "# Feature 3: Magnitude-weighted distance\n",
    "data['weighted_distance'] = data['distance_from_source'] * data['source_magnitude']\n",
    "\n",
    "# Feature 4: Duration of the seismic signal\n",
    "data['signal_duration'] = data['minute'] * 60 + data['second']\n",
    "\n",
    "\n",
    "# Drop the original columns used for feature engineering\n",
    "data = data.drop(['hour', 'minute', 'second'], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# Convert categorical columns to numerical representations\n",
    "categorical_columns = ['network_code', 'receiver_code', 'time_of_day']\n",
    "for column in categorical_columns:\n",
    "    data[column] = pd.Categorical(data[column])\n",
    "    data[column] = data[column].cat.codes\n",
    "\n",
    "    \n",
    "# Create a graph from the data\n",
    "graph = StellarGraph.from_pandas(data, node_features=[\"receiver_latitude\", \"receiver_longitude\",\n",
    "                                                       \"receiver_elevation_m\", \"source_latitude\",\n",
    "                                                       \"source_longitude\", \"source_depth_km\",\n",
    "                                                       \"source_magnitude\"],\n",
    "                                 edge_features=[\"distance_from_source\", \"weighted_distance\"],\n",
    "                                 node_type_default=\"receiver_code\", edge_type_default=\"trace_category\")\n",
    "\n",
    "# Train-test split\n",
    "X = data.drop(['trace_category', 'target'], axis=1)\n",
    "y = data['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# Convert data to StellarGraph instances\n",
    "G_train = graph.node_features(X_train_scaled)\n",
    "G_test = graph.node_features(X_test_scaled)\n",
    "\n",
    "\n",
    "# Build the GNN model\n",
    "model = models.Sequential()\n",
    "model.add(GCN(layer_sizes=[32], activations=[\"relu\"], generator=graph, dropout=0.5))\n",
    "model.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "model.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "# Define callbacks (e.g., early stopping to prevent overfitting)\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.fit(G_train, y_train, epochs=50, batch_size=64, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(G_test, y_test)\n",
    "print(f'Test Accuracy: {test_accuracy}')\n",
    "\n",
    "\n",
    "# Make predictions for user input\n",
    "def predict_earthquake_probability(user_input):\n",
    "    # Process user input (similar to preprocessing steps above)\n",
    "    user_input = pd.DataFrame(user_input, index=[0])\n",
    "    \n",
    "    # Feature engineering for user input\n",
    "    \n",
    "    # Scaling\n",
    "    user_input_scaled = scaler.transform(user_input)\n",
    "    \n",
    "    # Convert to StellarGraph instances\n",
    "    G_user_input = graph.node_features(user_input_scaled)\n",
    "    \n",
    "    # Make prediction\n",
    "    probability = model.predict(G_user_input)\n",
    "    \n",
    "    return probability[0][0]\n",
    "\n",
    "\n",
    "# Example usage\n",
    "user_location_input = {\n",
    "    'network_code': 'XYZ',\n",
    "    'receiver_code': 'ABC',\n",
    "    'receiver_latitude': 37.7749,\n",
    "    'receiver_longitude': -122.4194,\n",
    "    'receiver_elevation_m': 10.0,\n",
    "    'source_latitude': 34.0522,\n",
    "    'source_longitude': -118.2437,\n",
    "    'source_depth_km': 10.0,\n",
    "    'source_magnitude': 5.0,\n",
    "    'year': 2024,\n",
    "    'month': 2,\n",
    "    'day': 5,\n",
    "    'time_of_day': 'morning'\n",
    "}\n",
    "\n",
    "predicted_probability = predict_earthquake_probability(user_location_input)\n",
    "print(f'Predicted Probability of Earthquake: {predicted_probability}')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fa70d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ca6fc00",
   "metadata": {},
   "source": [
    "2nd try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af42b8ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import pandas as pd\\nfrom sklearn.preprocessing import MinMaxScaler\\n\\n# Load the dataset\\ndata = pd.read_csv(\"D:\\\\APURAV\\\\K. K. Wagh\\\\Study\\\\BE\\\\Semester VII\\\\Final Year Project Sem VII\\\\dataset\\\\STEAD\\\\merge.csv\")\\n\\n# Explore the dataset\\nprint(data.head())\\nprint(data.info())\\nprint(data.describe())\\n\\n# Handle missing values\\ndata.dropna(inplace=True)\\n\\n# Handle outliers (if necessary)\\n# Perform data consistency checks and corrections\\n\\n# Feature engineering\\n# Example:\\n# Convert source_origin_time to datetime\\ndata[\\'source_origin_time\\'] = pd.to_datetime(data[\\'source_origin_time\\'])\\n\\n# Scaling, encoding, dimensionality reduction (if necessary)\\n# Example:\\n# Feature scaling using Min-Max normalization\\nscaler = MinMaxScaler()\\nscaled_data = scaler.fit_transform(data[[\\'source_latitude\\', \\'source_longitude\\']])\\n\\n# Convert scaled_data back to a DataFrame\\nscaled_data_df = pd.DataFrame(scaled_data, columns=[\\'source_latitude\\', \\'source_longitude\\'])\\n\\n# Dimensionality reduction using PCA (Principal Component Analysis) or other methods if necessary\\n\\n# Save preprocessed data\\nscaled_data_df.to_csv(\"preprocessed_data.csv\", index=False)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"D:\\APURAV\\K. K. Wagh\\Study\\BE\\Semester VII\\Final Year Project Sem VII\\dataset\\STEAD\\merge.csv\")\n",
    "\n",
    "# Explore the dataset\n",
    "print(data.head())\n",
    "print(data.info())\n",
    "print(data.describe())\n",
    "\n",
    "# Handle missing values\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Handle outliers (if necessary)\n",
    "# Perform data consistency checks and corrections\n",
    "\n",
    "# Feature engineering\n",
    "# Example:\n",
    "# Convert source_origin_time to datetime\n",
    "data['source_origin_time'] = pd.to_datetime(data['source_origin_time'])\n",
    "\n",
    "# Scaling, encoding, dimensionality reduction (if necessary)\n",
    "# Example:\n",
    "# Feature scaling using Min-Max normalization\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data[['source_latitude', 'source_longitude']])\n",
    "\n",
    "# Convert scaled_data back to a DataFrame\n",
    "scaled_data_df = pd.DataFrame(scaled_data, columns=['source_latitude', 'source_longitude'])\n",
    "\n",
    "# Dimensionality reduction using PCA (Principal Component Analysis) or other methods if necessary\n",
    "\n",
    "# Save preprocessed data\n",
    "scaled_data_df.to_csv(\"preprocessed_data.csv\", index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "509324e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Implement GNN architecture using TensorFlow or PyTorch\\n# Example:\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\n\\nclass GNN(nn.Module):\\n    def __init__(self, input_dim, hidden_dim, output_dim):\\n        super(GNN, self).__init__()\\n        # Define GNN layers\\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\\n        self.fc2 = nn.Linear(hidden_dim, output_dim)\\n\\n    def forward(self, x):\\n        # Define forward pass\\n        x = F.relu(self.fc1(x))\\n        x = self.fc2(x)\\n        return x\\n\\n# Instantiate GNN model\\ninput_dim = 2  # Example: latitude and longitude\\nhidden_dim = 64\\noutput_dim = 1  # Probability of earthquake occurrence\\nmodel = GNN(input_dim, hidden_dim, output_dim)\\n\\n# Define loss function and optimizer\\ncriterion = nn.MSELoss()\\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Implement GNN architecture using TensorFlow or PyTorch\n",
    "# Example:\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GNN, self).__init__()\n",
    "        # Define GNN layers\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Define forward pass\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate GNN model\n",
    "input_dim = 2  # Example: latitude and longitude\n",
    "hidden_dim = 64\n",
    "output_dim = 1  # Probability of earthquake occurrence\n",
    "model = GNN(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e40b5e0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\n# Visualize seismic data distributions\\nsns.pairplot(data[['source_latitude', 'source_longitude', 'source_depth_km']])\\nplt.show()\\n\\n# Display GNN architecture diagrams (if necessary)\\n\\n# Use interactive plots for seismic waveforms and earthquake characteristics (if necessary)\\n# Example: Plot seismic waveforms over time using Plotly or Bokeh\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Visualize seismic data distributions\n",
    "sns.pairplot(data[['source_latitude', 'source_longitude', 'source_depth_km']])\n",
    "plt.show()\n",
    "\n",
    "# Display GNN architecture diagrams (if necessary)\n",
    "\n",
    "# Use interactive plots for seismic waveforms and earthquake characteristics (if necessary)\n",
    "# Example: Plot seismic waveforms over time using Plotly or Bokeh\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293883bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5213d8b",
   "metadata": {},
   "source": [
    "3rd try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "639f2f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import pandas as pd\\nimport numpy as np\\n\\n# Load the dataset\\ndata = pd.read_csv(\"D:\\\\APURAV\\\\K. K. Wagh\\\\Study\\\\BE\\\\Semester VII\\\\Final Year Project Sem VII\\\\dataset\\\\STEAD\\\\merge.csv\")\\n\\n# Drop unnecessary columns\\ndata.drop([\\'network_code\\', \\'receiver_code\\', \\'receiver_type\\', \\'receiver_elevation_m\\',\\n           \\'p_status\\', \\'p_weight\\', \\'p_travel_sec\\', \\'s_status\\', \\'s_weight\\', \\'source_id\\',\\n           \\'source_origin_uncertainty_sec\\', \\'source_error_sec\\', \\'source_gap_deg\\',\\n           \\'source_horizontal_uncertainty_km\\', \\'source_depth_uncertainty_km\\',\\n           \\'source_magnitude_type\\', \\'source_magnitude_author\\', \\'source_mechanism_strike_dip_rake\\',\\n           \\'source_distance_deg\\', \\'trace_start_time\\', \\'trace_category\\', \\'trace_name\\'], axis=1, inplace=True)\\n\\n# Convert timestamp columns to datetime objects\\ndata[\\'source_origin_time\\'] = pd.to_datetime(data[\\'source_origin_time\\'])\\n\\n# Extract features from timestamp columns\\ndata[\\'hour\\'] = data[\\'source_origin_time\\'].dt.hour\\ndata[\\'minute\\'] = data[\\'source_origin_time\\'].dt.minute\\ndata[\\'second\\'] = data[\\'source_origin_time\\'].dt.second\\n\\n# Feature scaling\\nfrom sklearn.preprocessing import StandardScaler\\nscaler = StandardScaler()\\nscaled_features = scaler.fit_transform(data[[\\'hour\\', \\'minute\\', \\'second\\']])\\ndata[[\\'hour\\', \\'minute\\', \\'second\\']] = scaled_features\\n\\n# Handle missing values\\ndata.fillna(0, inplace=True)\\n\\n# Encode categorical variables if needed\\n\\n# Final processed dataset\\nprocessed_data = data.copy()\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"D:\\APURAV\\K. K. Wagh\\Study\\BE\\Semester VII\\Final Year Project Sem VII\\dataset\\STEAD\\merge.csv\")\n",
    "\n",
    "# Drop unnecessary columns\n",
    "data.drop(['network_code', 'receiver_code', 'receiver_type', 'receiver_elevation_m',\n",
    "           'p_status', 'p_weight', 'p_travel_sec', 's_status', 's_weight', 'source_id',\n",
    "           'source_origin_uncertainty_sec', 'source_error_sec', 'source_gap_deg',\n",
    "           'source_horizontal_uncertainty_km', 'source_depth_uncertainty_km',\n",
    "           'source_magnitude_type', 'source_magnitude_author', 'source_mechanism_strike_dip_rake',\n",
    "           'source_distance_deg', 'trace_start_time', 'trace_category', 'trace_name'], axis=1, inplace=True)\n",
    "\n",
    "# Convert timestamp columns to datetime objects\n",
    "data['source_origin_time'] = pd.to_datetime(data['source_origin_time'])\n",
    "\n",
    "# Extract features from timestamp columns\n",
    "data['hour'] = data['source_origin_time'].dt.hour\n",
    "data['minute'] = data['source_origin_time'].dt.minute\n",
    "data['second'] = data['source_origin_time'].dt.second\n",
    "\n",
    "# Feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(data[['hour', 'minute', 'second']])\n",
    "data[['hour', 'minute', 'second']] = scaled_features\n",
    "\n",
    "# Handle missing values\n",
    "data.fillna(0, inplace=True)\n",
    "\n",
    "# Encode categorical variables if needed\n",
    "\n",
    "# Final processed dataset\n",
    "processed_data = data.copy()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98a31f2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\nfrom torch_geometric.nn import GCNConv\\n\\nclass GraphConvolutionalNetwork(nn.Module):\\n    def __init__(self, input_dim, hidden_dim, output_dim):\\n        super(GraphConvolutionalNetwork, self).__init__()\\n        self.conv1 = GCNConv(input_dim, hidden_dim)\\n        self.conv2 = GCNConv(hidden_dim, output_dim)\\n\\n    def forward(self, data):\\n        x, edge_index = data.x, data.edge_index\\n        x = F.relu(self.conv1(x, edge_index))\\n        x = F.relu(self.conv2(x, edge_index))\\n        return x\\n\\n# Initialize the GNN model\\ninput_dim = len(processed_data.columns)  # Adjust based on your processed data\\nhidden_dim = 64\\noutput_dim = 1  # Adjust based on your prediction task\\nmodel = GraphConvolutionalNetwork(input_dim, hidden_dim, output_dim)\\n\\n# Define loss function and optimizer\\ncriterion = nn.MSELoss()\\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GraphConvolutionalNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GraphConvolutionalNetwork, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        return x\n",
    "\n",
    "# Initialize the GNN model\n",
    "input_dim = len(processed_data.columns)  # Adjust based on your processed data\n",
    "hidden_dim = 64\n",
    "output_dim = 1  # Adjust based on your prediction task\n",
    "model = GraphConvolutionalNetwork(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f40f0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9923bfd",
   "metadata": {},
   "source": [
    "4th try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df8fa810",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Apurav\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3444: DtypeWarning: Columns (7,11,13,14,15,18,19,20,21,22,24,25,26,30,31) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  network_code receiver_code receiver_type  receiver_latitude  \\\n",
      "0           TA          109C            HH            32.8889   \n",
      "1           TA          109C            HH            32.8889   \n",
      "2           TA          109C            HH            32.8889   \n",
      "3           TA          109C            HH            32.8889   \n",
      "4           TA          109C            HH            32.8889   \n",
      "\n",
      "   receiver_longitude  receiver_elevation_m  p_arrival_sample p_status  \\\n",
      "0           -117.1051                 150.0               NaN      NaN   \n",
      "1           -117.1051                 150.0               NaN      NaN   \n",
      "2           -117.1051                 150.0               NaN      NaN   \n",
      "3           -117.1051                 150.0               NaN      NaN   \n",
      "4           -117.1051                 150.0               NaN      NaN   \n",
      "\n",
      "   p_weight  p_travel_sec  ...  source_magnitude_author  \\\n",
      "0       NaN           NaN  ...                      NaN   \n",
      "1       NaN           NaN  ...                      NaN   \n",
      "2       NaN           NaN  ...                      NaN   \n",
      "3       NaN           NaN  ...                      NaN   \n",
      "4       NaN           NaN  ...                      NaN   \n",
      "\n",
      "  source_mechanism_strike_dip_rake  source_distance_deg source_distance_km  \\\n",
      "0                              NaN                  NaN                NaN   \n",
      "1                              NaN                  NaN                NaN   \n",
      "2                              NaN                  NaN                NaN   \n",
      "3                              NaN                  NaN                NaN   \n",
      "4                              NaN                  NaN                NaN   \n",
      "\n",
      "  back_azimuth_deg snr_db  coda_end_sample     trace_start_time  \\\n",
      "0              NaN    NaN              NaN  2015-10-21 05:55:00   \n",
      "1              NaN    NaN              NaN  2015-11-06 14:50:00   \n",
      "2              NaN    NaN              NaN  2015-11-07 02:20:00   \n",
      "3              NaN    NaN              NaN  2015-11-14 05:15:00   \n",
      "4              NaN    NaN              NaN  2015-12-25 18:50:00   \n",
      "\n",
      "  trace_category               trace_name  \n",
      "0          noise  109C.TA_201510210555_NO  \n",
      "1          noise  109C.TA_201511061450_NO  \n",
      "2          noise  109C.TA_201511070220_NO  \n",
      "3          noise  109C.TA_201511140515_NO  \n",
      "4          noise  109C.TA_201512251850_NO  \n",
      "\n",
      "[5 rows x 35 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Data Understanding and Preprocessing\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"D:\\APURAV\\K. K. Wagh\\Study\\BE\\Semester VII\\Final Year Project Sem VII\\dataset\\STEAD\\merge.csv\")\n",
    "\n",
    "# Review the dataset\n",
    "print(data.head())\n",
    "\n",
    "# Handle missing values\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Handle outliers\n",
    "# You may use techniques like Z-score, IQR, or domain-specific knowledge\n",
    "\n",
    "# Feature engineering\n",
    "# Extract relevant features\n",
    "# Example: Extracting seismic waveforms into separate arrays\n",
    "seismic_waveforms = data[['trace_start_time', 'trace_category', 'trace_name']]\n",
    "\n",
    "# Data encoding and scaling\n",
    "# Example: Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "data[['source_magnitude', 'source_depth_km']] = scaler.fit_transform(data[['source_magnitude', 'source_depth_km']])\n",
    "\n",
    "# Dimensionality reduction\n",
    "# You may use techniques like PCA if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "035cd1b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_44088/1297890355.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# Instantiate the GNN model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0minput_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Adjust input dimension based on the number of features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[0mhidden_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0moutput_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m  \u001b[1;31m# Assuming binary classification for earthquake prediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GNNModel, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        \n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Instantiate the GNN model\n",
    "input_dim = len(X.columns)  # Adjust input dimension based on the number of features\n",
    "hidden_dim = 64\n",
    "output_dim = 2  # Assuming binary classification for earthquake prediction\n",
    "model = GNNModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee29db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Visualize data distributions, feature relationships, model predictions, etc.\n",
    "\n",
    "# For example, visualize the correlation matrix of features\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(data.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Visualize model performance metrics (e.g., accuracy, loss) during training\n",
    "def plot_metrics(train_losses, val_losses, train_accuracies, val_accuracies):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.plot(epochs, train_losses, label='Train Loss')\n",
    "    plt.plot(epochs, val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs, train_accuracies, label='Train Accuracy')\n",
    "    plt.plot(epochs, val_accuracies, label='Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Usage: Call plot_metrics with training and validation metrics lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b568ca17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=10):\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accuracies, val_accuracies = [], []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        \n",
    "        for data in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, data.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_train += data.y.size(0)\n",
    "            correct_train += (predicted == data.y).sum().item()\n",
    "        \n",
    "        train_losses.append(running_loss / len(train_loader))\n",
    "        train_accuracies.append(correct_train / total_train)\n",
    "        \n",
    "        val_loss, val_acc = evaluate_model(model, criterion, val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "        \n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "              f'Train Loss: {train_losses[-1]:.4f}, Train Acc: {train_accuracies[-1]:.4f}, '\n",
    "              f'Val Loss: {val_losses[-1]:.4f}, Val Acc: {val_accuracies[-1]:.4f}')\n",
    "    \n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate_model(model, criterion, data_loader):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, data.y)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += data.y.size(0)\n",
    "            correct += (predicted == data.y).sum().item()\n",
    "    \n",
    "    return running_loss / len(data_loader), correct / total\n",
    "\n",
    "# Make predictions\n",
    "def predict(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(data)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "    return predicted\n",
    "\n",
    "# Usage: train the model, evaluate it on validation data, and make predictions on test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23767db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "\n",
    "\n",
    "# Assuming X is your DataFrame with features\n",
    "# Identify categorical columns\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Perform feature hashing on categorical columns\n",
    "hasher = FeatureHasher(n_features=10, input_type='string')\n",
    "X_hashed = hasher.transform(X[categorical_cols].astype(str))\n",
    "\n",
    "# Convert hashed features to a DataFrame\n",
    "X_hashed_df = pd.DataFrame(X_hashed.toarray())\n",
    "\n",
    "# Concatenate hashed features with numerical features\n",
    "X_processed = pd.concat([X.drop(columns=categorical_cols), X_hashed_df], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# Assuming X_processed is your DataFrame with processed features\n",
    "# Check the dimensions of X_processed and y\n",
    "print(\"Dimensions of X_processed:\", X_processed.shape)\n",
    "print(\"Dimensions of y:\", y.shape)\n",
    "\n",
    "# Find the indices of rows that are present in X_processed but not in y\n",
    "missing_indices = set(X_processed.index) - set(y.index)\n",
    "print(\"Missing indices:\", missing_indices)\n",
    "\n",
    "# Find the indices of rows that are present in y but not in X_processed\n",
    "extra_indices = set(y.index) - set(X_processed.index)\n",
    "print(\"Extra indices:\", extra_indices)\n",
    "\n",
    "# Check if the number of samples in X_processed and y is consistent\n",
    "if len(X_processed) != len(y):\n",
    "    # If there are extra indices in y, drop those rows\n",
    "    if extra_indices:\n",
    "        print(\"Dropping extra indices in y:\", extra_indices)\n",
    "        y.drop(index=extra_indices, inplace=True)\n",
    "    \n",
    "    # If there are missing indices in y, drop those rows from X_processed\n",
    "    if missing_indices:\n",
    "        print(\"Dropping missing indices in X_processed:\", missing_indices)\n",
    "        X_processed.drop(index=missing_indices, inplace=True)\n",
    "    \n",
    "    # Check again for consistency\n",
    "    if len(X_processed) != len(y):\n",
    "        raise ValueError(\"Number of samples in features and target are not consistent!\")\n",
    "\n",
    "# Ensure that the indices of X_processed and y are aligned\n",
    "X_processed.reset_index(drop=True, inplace=True)\n",
    "y.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Concatenate X_processed and y along the columns axis\n",
    "#data = pd.concat([X_processed, y], axis=1)\n",
    "\n",
    "# Check the dimensions of the concatenated DataFrame\n",
    "print(\"Dimensions of concatenated data:\", data.shape)\n",
    "\n",
    "# Drop columns 'trace_start_time' and 'trace_name' from the concatenated DataFrame\n",
    "data.drop(columns=['trace_start_time', 'trace_name'], inplace=True)\n",
    "\n",
    "# Split data into features and target\n",
    "X = data.drop(columns=['trace_category'])\n",
    "y = data['trace_category']\n",
    "\n",
    "\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_processed, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Convert data to appropriate numeric types\n",
    "X_train_values = X_train.values.astype(np.float32)\n",
    "y_train_values = y_train.values.astype(np.long)\n",
    "X_val_values = X_val.values.astype(np.float32)\n",
    "y_val_values = y_val.values.astype(np.long)\n",
    "X_test_values = X_test.values.astype(np.float32)\n",
    "y_test_values = y_test.values.astype(np.long)\n",
    "\n",
    "# Assuming train_edge_index, val_edge_index, and test_edge_index are properly defined\n",
    "# Define the data objects\n",
    "train_data = Data.Data(x=torch.tensor(X_train_values),\n",
    "                       y=torch.tensor(y_train_values),\n",
    "                       edge_index=torch.tensor(train_edge_index, dtype=torch.long))\n",
    "val_data = Data.Data(x=torch.tensor(X_val_values),\n",
    "                     y=torch.tensor(y_val_values),\n",
    "                     edge_index=torch.tensor(val_edge_index, dtype=torch.long))\n",
    "test_data = Data.Data(x=torch.tensor(X_test_values),\n",
    "                      y=torch.tensor(y_test_values),\n",
    "                      edge_index=torch.tensor(test_edge_index, dtype=torch.long))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f3cdd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
