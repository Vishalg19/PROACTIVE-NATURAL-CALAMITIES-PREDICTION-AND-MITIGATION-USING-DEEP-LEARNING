{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the earthquake dataset using a library like pandas in \n",
    "python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_18952\\3439843406.py:7: DtypeWarning: Columns (7,11,13,14,24,25,26,30,31) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  earthquake_data = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  network_code receiver_code receiver_type  receiver_latitude  \\\n",
      "0           TA          109C            HH            32.8889   \n",
      "1           TA          109C            HH            32.8889   \n",
      "2           TA          109C            HH            32.8889   \n",
      "3           TA          109C            HH            32.8889   \n",
      "4           TA          109C            HH            32.8889   \n",
      "\n",
      "   receiver_longitude  receiver_elevation_m  p_arrival_sample p_status  \\\n",
      "0           -117.1051                 150.0               NaN      NaN   \n",
      "1           -117.1051                 150.0               NaN      NaN   \n",
      "2           -117.1051                 150.0               NaN      NaN   \n",
      "3           -117.1051                 150.0               NaN      NaN   \n",
      "4           -117.1051                 150.0               NaN      NaN   \n",
      "\n",
      "   p_weight  p_travel_sec  ...  source_magnitude_author  \\\n",
      "0       NaN           NaN  ...                      NaN   \n",
      "1       NaN           NaN  ...                      NaN   \n",
      "2       NaN           NaN  ...                      NaN   \n",
      "3       NaN           NaN  ...                      NaN   \n",
      "4       NaN           NaN  ...                      NaN   \n",
      "\n",
      "  source_mechanism_strike_dip_rake  source_distance_deg source_distance_km  \\\n",
      "0                              NaN                  NaN                NaN   \n",
      "1                              NaN                  NaN                NaN   \n",
      "2                              NaN                  NaN                NaN   \n",
      "3                              NaN                  NaN                NaN   \n",
      "4                              NaN                  NaN                NaN   \n",
      "\n",
      "  back_azimuth_deg  snr_db  coda_end_sample     trace_start_time  \\\n",
      "0              NaN     NaN              NaN  2015-10-21 05:55:00   \n",
      "1              NaN     NaN              NaN  2015-11-06 14:50:00   \n",
      "2              NaN     NaN              NaN  2015-11-07 02:20:00   \n",
      "3              NaN     NaN              NaN  2015-11-14 05:15:00   \n",
      "4              NaN     NaN              NaN  2015-12-25 18:50:00   \n",
      "\n",
      "   trace_category               trace_name  \n",
      "0           noise  109C.TA_201510210555_NO  \n",
      "1           noise  109C.TA_201511061450_NO  \n",
      "2           noise  109C.TA_201511070220_NO  \n",
      "3           noise  109C.TA_201511140515_NO  \n",
      "4           noise  109C.TA_201512251850_NO  \n",
      "\n",
      "[5 rows x 35 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1265657 entries, 0 to 1265656\n",
      "Data columns (total 35 columns):\n",
      " #   Column                            Non-Null Count    Dtype  \n",
      "---  ------                            --------------    -----  \n",
      " 0   network_code                      1265613 non-null  object \n",
      " 1   receiver_code                     1265657 non-null  object \n",
      " 2   receiver_type                     1265657 non-null  object \n",
      " 3   receiver_latitude                 1265657 non-null  float64\n",
      " 4   receiver_longitude                1265657 non-null  float64\n",
      " 5   receiver_elevation_m              1265657 non-null  float64\n",
      " 6   p_arrival_sample                  1030231 non-null  float64\n",
      " 7   p_status                          1030231 non-null  object \n",
      " 8   p_weight                          1030057 non-null  float64\n",
      " 9   p_travel_sec                      1030231 non-null  float64\n",
      " 10  s_arrival_sample                  1030231 non-null  float64\n",
      " 11  s_status                          1030231 non-null  object \n",
      " 12  s_weight                          1030076 non-null  float64\n",
      " 13  source_id                         1030231 non-null  object \n",
      " 14  source_origin_time                1030231 non-null  object \n",
      " 15  source_origin_uncertainty_sec     140294 non-null   float64\n",
      " 16  source_latitude                   1030231 non-null  float64\n",
      " 17  source_longitude                  1030231 non-null  float64\n",
      " 18  source_error_sec                  459503 non-null   float64\n",
      " 19  source_gap_deg                    380817 non-null   float64\n",
      " 20  source_horizontal_uncertainty_km  440738 non-null   float64\n",
      " 21  source_depth_km                   1030182 non-null  float64\n",
      " 22  source_depth_uncertainty_km       369423 non-null   float64\n",
      " 23  source_magnitude                  1030231 non-null  float64\n",
      " 24  source_magnitude_type             1030231 non-null  object \n",
      " 25  source_magnitude_author           250178 non-null   object \n",
      " 26  source_mechanism_strike_dip_rake  6028 non-null     object \n",
      " 27  source_distance_deg               1030231 non-null  float64\n",
      " 28  source_distance_km                1030231 non-null  float64\n",
      " 29  back_azimuth_deg                  1030231 non-null  float64\n",
      " 30  snr_db                            1030231 non-null  object \n",
      " 31  coda_end_sample                   1030231 non-null  object \n",
      " 32  trace_start_time                  1265657 non-null  object \n",
      " 33  trace_category                    1265657 non-null  object \n",
      " 34  trace_name                        1265657 non-null  object \n",
      "dtypes: float64(20), object(15)\n",
      "memory usage: 338.0+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace 'your_dataset.csv' with the path to your earthquake dataset file\n",
    "file_path = 'merge.csv'\n",
    "\n",
    "# Load the dataset\n",
    "earthquake_data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "print(earthquake_data.head())\n",
    "print(earthquake_data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate the feature attributes like latitude, \n",
    "longitude, depth, hour and the target attribute magnitude.... but the columns of the dataset are: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Preview:\n",
      "   source_latitude  source_longitude  source_depth_km  hour\n",
      "0              NaN               NaN              NaN   NaN\n",
      "1              NaN               NaN              NaN   NaN\n",
      "2              NaN               NaN              NaN   NaN\n",
      "3              NaN               NaN              NaN   NaN\n",
      "4              NaN               NaN              NaN   NaN\n",
      "\n",
      "Target Preview:\n",
      "0   NaN\n",
      "1   NaN\n",
      "2   NaN\n",
      "3   NaN\n",
      "4   NaN\n",
      "Name: source_magnitude, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming earthquake_data is your DataFrame\n",
    "# Correct any column name discrepancies and ensure you've loaded the data correctly.\n",
    "\n",
    "# Convert 'source_origin_time' to datetime format and extract the hour directly into the features DataFrame\n",
    "earthquake_data['hour'] = pd.to_datetime(earthquake_data['source_origin_time']).dt.hour\n",
    "\n",
    "# Now select your features including the newly created 'hour'\n",
    "features = earthquake_data[['source_latitude', 'source_longitude', 'source_depth_km', 'hour']]\n",
    "\n",
    "# Target remains the same\n",
    "target = earthquake_data['source_magnitude']\n",
    "\n",
    "print(\"Features Preview:\")\n",
    "print(features.head())\n",
    "print(\"\\nTarget Preview:\")\n",
    "print(target.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop missing values from the dataset before separating features and the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Preview:\n",
      "        source_latitude  source_longitude  source_depth_km  hour\n",
      "235426          33.7496         -117.4938             0.45    15\n",
      "235427          32.7077         -116.0446             9.20    15\n",
      "235428          32.7253         -116.0348            12.66    16\n",
      "235429          32.7063         -116.0241            11.50    13\n",
      "235430          31.9679         -117.1944             7.26    10\n",
      "\n",
      "Target Preview:\n",
      "235426    3.6\n",
      "235427    4.3\n",
      "235428    3.6\n",
      "235429    3.8\n",
      "235430    3.6\n",
      "Name: source_magnitude, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_18952\\1281162062.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleaned_data['hour'] = pd.to_datetime(cleaned_data['source_origin_time']).dt.hour\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming earthquake_data is your DataFrame\n",
    "# Step 1: Drop rows with any missing values in the specified columns\n",
    "cleaned_data = earthquake_data.dropna(subset=['source_latitude', 'source_longitude', 'source_depth_km', 'source_origin_time', 'source_magnitude'])\n",
    "\n",
    "# Now that the data is cleaned, you can safely extract the hour from 'source_origin_time'\n",
    "# Convert 'source_origin_time' to datetime and extract the hour\n",
    "cleaned_data['hour'] = pd.to_datetime(cleaned_data['source_origin_time']).dt.hour\n",
    "\n",
    "# Step 2: Separate features and target in the cleaned dataset\n",
    "features = cleaned_data[['source_latitude', 'source_longitude', 'source_depth_km', 'hour']]\n",
    "target = cleaned_data['source_magnitude']\n",
    "\n",
    "print(\"Features Preview:\")\n",
    "print(features.head())\n",
    "print(\"\\nTarget Preview:\")\n",
    "print(target.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The StandardScaler function is used to normalize the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Features Preview:\n",
      "   source_latitude  source_longitude  source_depth_km      hour\n",
      "0        -0.486272         -0.018463        -0.627976  0.550654\n",
      "1        -0.561782          0.016048        -0.266589  0.550654\n",
      "2        -0.560506          0.016281        -0.123686  0.697211\n",
      "3        -0.561883          0.016536        -0.171596  0.257540\n",
      "4        -0.615397         -0.011333        -0.346714 -0.182130\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming 'features' is your DataFrame containing the feature attributes\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to your data and transform it\n",
    "normalized_features = scaler.fit_transform(features)\n",
    "\n",
    "# If you want to convert it back to a DataFrame\n",
    "normalized_features_df = pd.DataFrame(normalized_features, columns=features.columns)\n",
    "\n",
    "print(\"Normalized Features Preview:\")\n",
    "print(normalized_features_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the dataset separate the dataset into training and testing \n",
    "sets using train_test_split function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (824145, 4)\n",
      "Shape of X_test: (206037, 4)\n",
      "Shape of y_train: (824145,)\n",
      "Shape of y_test: (206037,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming 'normalized_features' and 'target' are your features and target variables\n",
    "# Split the dataset into 80% training and 20% testing, you can adjust the 'test_size' parameter as needed\n",
    "X_train, X_test, y_train, y_test = train_test_split(normalized_features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Now you have X_train (features for training), X_test (features for testing),\n",
    "# y_train (target for training), and y_test (target for testing)\n",
    "\n",
    "# Display the shapes of the resulting sets\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 19\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Convert the edge_index to PyTorch tensor with shape [2, num_edges]\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# For simplicity, assuming a fully connected graph, adjust accordingly based on your actual graph structure\u001b[39;00m\n\u001b[0;32m     17\u001b[0m num_nodes \u001b[38;5;241m=\u001b[39m X_train_tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     18\u001b[0m edge_index_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\n\u001b[1;32m---> 19\u001b[0m     [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_nodes) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_nodes)],\n\u001b[0;32m     20\u001b[0m     [j \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_nodes) \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_nodes)]\n\u001b[0;32m     21\u001b[0m ], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Define a simple GNN model using GraphConv layers\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mGNNModel\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GraphConv\n",
    "\n",
    "# Assuming you have already loaded and pre-processed your data into X_train, y_train, X_test, y_test\n",
    "\n",
    "# Convert features and target to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "# Convert the edge_index to PyTorch tensor with shape [2, num_edges]\n",
    "# For simplicity, assuming a fully connected graph, adjust accordingly based on your actual graph structure\n",
    "num_nodes = X_train_tensor.shape[0]\n",
    "edge_index_tensor = torch.tensor([\n",
    "    [i for i in range(num_nodes) for _ in range(num_nodes)],\n",
    "    [j for _ in range(num_nodes) for j in range(num_nodes)]\n",
    "], dtype=torch.long)\n",
    "\n",
    "# Define a simple GNN model using GraphConv layers\n",
    "class GNNModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(GNNModel, self).__init__()\n",
    "        self.conv1 = GraphConv(input_size, 16)\n",
    "        self.conv2 = GraphConv(16, 1)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = torch.relu(self.conv1(x, edge_index))\n",
    "        x = torch.relu(self.conv2(x, edge_index))\n",
    "        return x\n",
    "\n",
    "# Create a GNN model\n",
    "input_size = X_train_tensor.shape[1]\n",
    "model = GNNModel(input_size)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Convert data to PyTorch Geometric Data objects\n",
    "train_data = Data(x=X_train_tensor, edge_index=edge_index_tensor, y=y_train_tensor)\n",
    "test_data = Data(x=X_test_tensor, edge_index=edge_index_tensor, y=y_test_tensor)\n",
    "\n",
    "# Define data loaders\n",
    "train_loader = DataLoader([train_data], batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader([test_data], batch_size=64, shuffle=False)\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch.x, batch.edge_index)\n",
    "        loss = criterion(out, batch.y.view(-1, 1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        predictions = model(batch.x, batch.edge_index)\n",
    "        test_loss = criterion(predictions, batch.y.view(-1, 1))\n",
    "\n",
    "print(\"Test Loss:\", test_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch-geometric in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.5.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch-geometric) (4.66.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch-geometric) (1.26.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch-geometric) (1.12.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch-geometric) (2024.2.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from torch-geometric) (3.1.2)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch-geometric) (3.9.3)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from torch-geometric) (2.31.0)\n",
      "Requirement already satisfied: pyparsing in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch-geometric) (3.1.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch-geometric) (1.4.1.post1)\n",
      "Requirement already satisfied: psutil>=5.8.0 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from torch-geometric) (5.9.8)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->torch-geometric) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->torch-geometric) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->torch-geometric) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->torch-geometric) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->torch-geometric) (1.9.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch-geometric) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from requests->torch-geometric) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from requests->torch-geometric) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from requests->torch-geometric) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from requests->torch-geometric) (2023.11.17)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn->torch-geometric) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn->torch-geometric) (3.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->torch-geometric) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
